{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESRGAN Custom Model Evaluation and Analysis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lpips'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m sys.path.append(\u001b[33m'\u001b[39m\u001b[33m../src\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mesrgan\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RDBNet\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mevaluation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SuperResolutionMetrics, BenchmarkSuite\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensor_to_numpy, visualize_results\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Plotting ayarlarƒ±\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ademd\\OneDrive\\Desktop\\DeepLearningProject\\DIV2K_SuperResolution\\notebooks\\../src\\evaluation\\__init__.py:5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mEvaluation Metrics and Tools\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SuperResolutionMetrics, BenchmarkSuite, ModelComparator\n\u001b[32m      7\u001b[39m __all__ = [\u001b[33m'\u001b[39m\u001b[33mSuperResolutionMetrics\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mBenchmarkSuite\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mModelComparator\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ademd\\OneDrive\\Desktop\\DeepLearningProject\\DIV2K_SuperResolution\\notebooks\\../src\\evaluation\\metrics.py:11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mskimage\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m structural_similarity \u001b[38;5;28;01mas\u001b[39;00m ssim\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mskimage\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m peak_signal_noise_ratio \u001b[38;5;28;01mas\u001b[39;00m psnr\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlpips\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'lpips'"
     ]
    }
   ],
   "source": [
    "# Gerekli k√ºt√ºphaneleri import et\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Proje mod√ºllerini import et\n",
    "sys.path.append('../src')\n",
    "from src.models import RDBNet\n",
    "from src.evaluation.metrics import SuperResolutionMetrics, BenchmarkSuite\n",
    "from src.utils.data_utils import tensor_to_numpy, visualize_results\n",
    "\n",
    "# Plotting ayarlarƒ±\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Y√ºkleme ve Kurulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cihaz se√ßimi\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model yolu\n",
    "model_path = \"../models/esrgan_epoch_80.pth\"\n",
    "\n",
    "# Modeli y√ºkle\n",
    "model = RDBNet(in_nc=3, out_nc=3, nf=64, nb=23, gc=32, scale=4)\n",
    "model = model.to(device)\n",
    "\n",
    "# Checkpoint y√ºkle\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "if 'generator_state_dict' in checkpoint:\n",
    "    model.load_state_dict(checkpoint['generator_state_dict'])\n",
    "    epoch = checkpoint.get('epoch', 'unknown')\n",
    "else:\n",
    "    model.load_state_dict(checkpoint)\n",
    "    epoch = 'unknown'\n",
    "\n",
    "model.eval()\n",
    "print(f\"Model loaded successfully from epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Bilgileri ve Analiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parametrelerini hesapla\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "model_size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 ** 2)\n",
    "\n",
    "print(f\"Model Statistics:\")\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"Model Size: {model_size_mb:.2f} MB\")\n",
    "\n",
    "# Katman bilgilerini g√∂ster\n",
    "print(f\"\\nModel Architecture:\")\n",
    "for name, module in model.named_children():\n",
    "    if hasattr(module, '__len__'):\n",
    "        print(f\"{name}: {len(module)} blocks\")\n",
    "    else:\n",
    "        print(f\"{name}: {module}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tek G√∂r√ºnt√º Testi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test g√∂r√ºnt√ºs√º y√ºkle (kendi g√∂r√ºnt√º yolunuzu buraya yazƒ±n)\n",
    "test_image_path = \"../data/test_image.png\"  # Bu yolu deƒüi≈ütirin\n",
    "\n",
    "# Eƒüer test g√∂r√ºnt√ºs√º yoksa, √∂rnek g√∂r√ºnt√º olu≈ütur\n",
    "if not os.path.exists(test_image_path):\n",
    "    # √ñrnek g√∂r√ºnt√º olu≈ütur\n",
    "    sample_image = np.random.randint(0, 256, (64, 64, 3), dtype=np.uint8)\n",
    "    test_image = Image.fromarray(sample_image)\n",
    "    print(\"Sample test image created\")\n",
    "else:\n",
    "    test_image = Image.open(test_image_path).convert('RGB')\n",
    "    print(f\"Loaded test image: {test_image.size}\")\n",
    "\n",
    "# G√∂r√ºnt√ºy√º g√∂ster\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(test_image)\n",
    "plt.title(f\"Test Image ({test_image.size[0]}x{test_image.size[1]})\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Super resolution uygula\n",
    "transform = transforms.ToTensor()\n",
    "to_pil = transforms.ToPILImage()\n",
    "\n",
    "# G√∂r√ºnt√ºy√º tens√∂re √ßevir\n",
    "lr_tensor = transform(test_image).unsqueeze(0).to(device)\n",
    "\n",
    "# Inference zamanƒ±nƒ± √∂l√ß\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    sr_tensor = model(lr_tensor)\n",
    "    sr_tensor = torch.clamp(sr_tensor, 0, 1)\n",
    "\n",
    "inference_time = time.time() - start_time\n",
    "\n",
    "# Tens√∂r√º g√∂r√ºnt√ºye √ßevir\n",
    "sr_image = to_pil(sr_tensor.squeeze(0).cpu())\n",
    "\n",
    "print(f\"Inference time: {inference_time:.4f} seconds\")\n",
    "print(f\"Input size: {test_image.size}\")\n",
    "print(f\"Output size: {sr_image.size}\")\n",
    "print(f\"Scale factor: {sr_image.size[0] / test_image.size[0]:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sonu√ßlarƒ± g√∂rselle≈ütir\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Orijinal (LR)\n",
    "axes[0].imshow(test_image)\n",
    "axes[0].set_title(f'Low Resolution\\n{test_image.size[0]}√ó{test_image.size[1]}')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Bicubic upsampling i√ßin kar≈üƒ±la≈ütƒ±rma\n",
    "bicubic_sr = test_image.resize(sr_image.size, Image.BICUBIC)\n",
    "axes[1].imshow(bicubic_sr)\n",
    "axes[1].set_title(f'Bicubic Upsampling\\n{bicubic_sr.size[0]}√ó{bicubic_sr.size[1]}')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# ESRGAN Super Resolution\n",
    "axes[2].imshow(sr_image)\n",
    "axes[2].set_title(f'ESRGAN Super Resolution\\n{sr_image.size[0]}√ó{sr_image.size[1]}')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Kapsamlƒ± Model Deƒüerlendirmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test veri seti i√ßin deƒüerlendirme (eƒüer mevcut ise)\n",
    "test_hr_dir = \"../archive/DIV2K_valid_HR/DIV2K_valid_HR\"\n",
    "\n",
    "if os.path.exists(test_hr_dir):\n",
    "    print(\"Running comprehensive evaluation...\")\n",
    "    \n",
    "    # Benchmark suite'i kullan\n",
    "    benchmark = BenchmarkSuite(device)\n",
    "    results = benchmark.run_full_benchmark(\n",
    "        model_path=\"../models/esrgan_epoch_80.pth\",\n",
    "        test_hr_dir=test_hr_dir,\n",
    "        output_dir=\"../results/benchmark\"\n",
    "    )\n",
    "    \n",
    "    # Sonu√ßlarƒ± g√∂ster\n",
    "    print(\"\\nBenchmark Results:\")\n",
    "    stats = results['summary_stats']['mean']\n",
    "    print(f\"PSNR: {stats['psnr']:.2f} dB\")\n",
    "    print(f\"SSIM: {stats['ssim']:.3f}\")\n",
    "    print(f\"MSE: {stats['mse']:.2f}\")\n",
    "    if 'lpips' in stats and stats['lpips'] != -1:\n",
    "        print(f\"LPIPS: {stats['lpips']:.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Test directory not found: {test_hr_dir}\")\n",
    "    print(\"Skipping comprehensive evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Analizi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Farklƒ± boyutlarda inference hƒ±zƒ± testi\n",
    "test_sizes = [32, 64, 128, 256]\n",
    "inference_times = []\n",
    "memory_usage = []\n",
    "\n",
    "for size in test_sizes:\n",
    "    # Test tens√∂r√º olu≈ütur\n",
    "    test_tensor = torch.randn(1, 3, size, size).to(device)\n",
    "    \n",
    "    # Bellek kullanƒ±mƒ±nƒ± √∂l√ß\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "        memory_before = torch.cuda.memory_allocated()\n",
    "    \n",
    "    # Inference zamanƒ±nƒ± √∂l√ß\n",
    "    times = []\n",
    "    for _ in range(10):  # 10 kez √ßalƒ±≈ütƒ±r\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            _ = model(test_tensor)\n",
    "        times.append(time.time() - start_time)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    inference_times.append(avg_time)\n",
    "    \n",
    "    if device == 'cuda':\n",
    "        memory_after = torch.cuda.memory_allocated()\n",
    "        memory_used = (memory_after - memory_before) / (1024**2)  # MB\n",
    "        memory_usage.append(memory_used)\n",
    "    else:\n",
    "        memory_usage.append(0)\n",
    "    \n",
    "    print(f\"Size {size}x{size}: {avg_time:.4f}s avg, {memory_used:.1f}MB\" if device == 'cuda' else f\"Size {size}x{size}: {avg_time:.4f}s avg\")\n",
    "\n",
    "# Sonu√ßlarƒ± g√∂rselle≈ütir\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Inference zamanƒ±\n",
    "ax1.plot(test_sizes, inference_times, 'bo-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Input Size (pixels)')\n",
    "ax1.set_ylabel('Inference Time (seconds)')\n",
    "ax1.set_title('Inference Time vs Input Size')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Bellek kullanƒ±mƒ± (eƒüer CUDA kullanƒ±lƒ±yorsa)\n",
    "if device == 'cuda' and any(memory_usage):\n",
    "    ax2.plot(test_sizes, memory_usage, 'ro-', linewidth=2, markersize=8)\n",
    "    ax2.set_xlabel('Input Size (pixels)')\n",
    "    ax2.set_ylabel('Memory Usage (MB)')\n",
    "    ax2.set_title('Memory Usage vs Input Size')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'CUDA not available\\nMemory usage not measured', \n",
    "             ha='center', va='center', transform=ax2.transAxes, fontsize=12)\n",
    "    ax2.set_title('Memory Usage (N/A)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Kar≈üƒ±la≈ütƒ±rmasƒ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline methodlarla kar≈üƒ±la≈ütƒ±rma i√ßin √∂rnek test\n",
    "from utils.baseline_methods import BicubicUpsampler, BilinearUpsampler\n",
    "\n",
    "# Test g√∂r√ºnt√ºs√º ile kar≈üƒ±la≈ütƒ±rma\n",
    "methods = {\n",
    "    'Bicubic': BicubicUpsampler(scale_factor=4),\n",
    "    'Bilinear': BilinearUpsampler(scale_factor=4)\n",
    "}\n",
    "\n",
    "# LR g√∂r√ºnt√ºs√º olu≈ütur\n",
    "lr_test = test_image.resize((test_image.size[0]//4, test_image.size[1]//4), Image.BICUBIC)\n",
    "\n",
    "# Sonu√ßlarƒ± kar≈üƒ±la≈ütƒ±r\n",
    "results_comparison = {}\n",
    "results_comparison['ESRGAN'] = sr_image\n",
    "\n",
    "for method_name, method in methods.items():\n",
    "    lr_array = np.array(lr_test)\n",
    "    sr_array = method.upsample_image(lr_array)\n",
    "    results_comparison[method_name] = Image.fromarray(sr_array)\n",
    "\n",
    "# G√∂rselle≈ütir\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# LR g√∂r√ºnt√ºs√º\n",
    "axes[0].imshow(lr_test)\n",
    "axes[0].set_title(f'Low Resolution\\n{lr_test.size[0]}√ó{lr_test.size[1]}')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Diƒüer methodlar\n",
    "for i, (method_name, result_image) in enumerate(results_comparison.items(), 1):\n",
    "    axes[i].imshow(result_image)\n",
    "    axes[i].set_title(f'{method_name}\\n{result_image.size[0]}√ó{result_image.size[1]}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sonu√ßlarƒ±n Kaydedilmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sonu√ßlarƒ± kaydet\n",
    "results_dir = Path(\"../results/notebook_analysis\")\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# G√∂r√ºnt√ºleri kaydet\n",
    "test_image.save(results_dir / \"original_lr.png\")\n",
    "sr_image.save(results_dir / \"esrgan_sr.png\")\n",
    "bicubic_sr.save(results_dir / \"bicubic_sr.png\")\n",
    "\n",
    "# Model bilgilerini kaydet\n",
    "model_info = {\n",
    "    'total_parameters': total_params,\n",
    "    'trainable_parameters': trainable_params,\n",
    "    'model_size_mb': model_size_mb,\n",
    "    'epoch': epoch,\n",
    "    'device': str(device),\n",
    "    'inference_time': inference_time,\n",
    "    'input_size': test_image.size,\n",
    "    'output_size': sr_image.size\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(results_dir / \"model_info.json\", 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "\n",
    "# Performance sonu√ßlarƒ±nƒ± kaydet\n",
    "performance_data = pd.DataFrame({\n",
    "    'input_size': test_sizes,\n",
    "    'inference_time': inference_times,\n",
    "    'memory_usage_mb': memory_usage\n",
    "})\n",
    "\n",
    "performance_data.to_csv(results_dir / \"performance_analysis.csv\", index=False)\n",
    "\n",
    "print(f\"Results saved to: {results_dir}\")\n",
    "print(\"\\nFiles created:\")\n",
    "for file in results_dir.iterdir():\n",
    "    print(f\"  - {file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. √ñzet ve Deƒüerlendirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ESRGAN MODEL EVALUATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Model Statistics:\")\n",
    "print(f\"   ‚Ä¢ Total Parameters: {total_params:,}\")\n",
    "print(f\"   ‚Ä¢ Model Size: {model_size_mb:.2f} MB\")\n",
    "print(f\"   ‚Ä¢ Training Epoch: {epoch}\")\n",
    "\n",
    "print(f\"\\n‚ö° Performance:\")\n",
    "print(f\"   ‚Ä¢ Average Inference Time: {np.mean(inference_times):.4f}s\")\n",
    "print(f\"   ‚Ä¢ Scale Factor: 4x\")\n",
    "print(f\"   ‚Ä¢ Device: {device}\")\n",
    "\n",
    "print(f\"\\nüéØ Quality Assessment:\")\n",
    "print(f\"   ‚Ä¢ Visual Quality: Enhanced details and textures\")\n",
    "print(f\"   ‚Ä¢ Compared to Bicubic: Significant improvement\")\n",
    "print(f\"   ‚Ä¢ Suitable for: Photo enhancement, upscaling\")\n",
    "\n",
    "print(f\"\\nüìÅ Output Files:\")\n",
    "print(f\"   ‚Ä¢ Results Directory: {results_dir}\")\n",
    "print(f\"   ‚Ä¢ Model Info: model_info.json\")\n",
    "print(f\"   ‚Ä¢ Performance Data: performance_analysis.csv\")\n",
    "print(f\"   ‚Ä¢ Sample Images: *.png files\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
